## Toxic Comment Classifier
The "Toxic Comment Classifier" is a data science project aimed at developing a robust system for identifying and categorizing toxic comments in online discussions. Leveraging machine learning and natural language processing (NLP) techniques, this project provides a comprehensive solution for mitigating harmful content in online communities. <br>

## Key Process

1. Data Exploration and Preparation:<br>
• Explore the dataset and analyze the distribution of each toxicity class.<br>
• Preprocess the text data by removing stopwords, punctuation and other irrelevant characters. Also, perform text normalization techniques such as stemming and lemmatization and can also use other preprocessing techniques like De-contraction, Chunking, etc. <br>
• Split the data into training and testing sets. <br> 

2. Feature Extraction:<br>
• Extract relevant features from the preprocessed text data, such as Bag-of-Words, TF-IDF, and word embeddings.<br>

3. Model Building:<br>
• Build various models, using Multi-label Classification Techniques like, Problem Transformation methods, ensemble algorithms, Adapted algorithms, Neural Networks.<br>
• Experiment with innovative models, such as neural networks, deep learning models, or ensemble methods, and compare their performance with traditional models.<br>
• Evaluate the performance of each model. Fine-tune the best performing model.<br>

4. Comparing Models and concluding:<br>
• Compare the best model and predict their toxicity classes.,<br>
• Build a flaskapi.<br>

Dataset:<br>
The Jigsaw Toxic Comment Classification Challenge dataset contains over 1.6 million comments from Wikipedia talk pages. The dataset is labeled with types of toxicity:<br>
i. toxic<br>
ii. severe_toxic<br>
iii. obscene<br>
iv. threat<br>
v. insult<br>
vi. identity_hate<br>

Data link: [Toxic Comment Classification](https://celebaltech-my.sharepoint.com/personal/anshi_gupta_celebaltech_com/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fanshi%5Fgupta%5Fcelebaltech%5Fcom%2FDocuments%2FData%2FToxic%20Comment%20Classification&ga=1)
<br>
